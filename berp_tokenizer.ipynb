{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: click in c:\\abhiraj\\nlpproject\\abhi_env\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\abhiraj\\nlpproject\\abhi_env\\lib\\site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in c:\\abhiraj\\nlpproject\\abhi_env\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\abhiraj\\nlpproject\\abhi_env\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.5 MB 262.6 kB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.0/1.5 MB 245.8 kB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.1/1.5 MB 416.7 kB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.1/1.5 MB 568.9 kB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.2/1.5 MB 765.3 kB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.4/1.5 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.5/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.7/1.5 MB 1.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.0/1.5 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 2.7 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "   ---------------------------------------- 0.0/301.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 301.8/301.8 kB 9.4 MB/s eta 0:00:00\n",
      "Installing collected packages: joblib, nltk\n",
      "Successfully installed joblib-1.4.2 nltk-3.8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers -q\n",
    "# %pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Abhiraj\\NLPProject\\abhi_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PdfParser:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "\n",
    "    def extract_text(self) -> str:\n",
    "        reader = PdfReader(self.filepath) \n",
    "        text = ''.join(page.extract_text() for page in reader.pages)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    extractor = PdfParser(r\"C:\\Abhiraj\\InvoiceParser\\Invoices\\ALOIS TECH PVT LTD -14.pdf\")\n",
    "    extracted_text = extractor.extract_text()\n",
    "    print(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize stop words, Load the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yourselves', 'this', 'these', 'those', 'being', 'your', 'itself', 'to', 'or', 'than', 'further', 'into', 'of', 'with', 'her', \"isn't\", 'weren', \"that'll\", \"she's\", \"mightn't\", 'more', 'off', 'whom', 'here', 'isn', 'are', 'hadn', \"you're\", 'couldn', 'that', 'so', 'in', 'ourselves', 'now', 'he', 'down', 'while', 'ma', 'have', 'were', 'does', 'been', 'from', 'my', 'then', 'own', \"you'll\", \"it's\", 'if', 'very', 'their', 'won', 'what', 'below', 're', 'through', \"you'd\", \"shan't\", 'over', 'not', 'hasn', 'only', 'ours', 'which', 'such', 'be', \"wasn't\", 'me', 'we', \"should've\", \"weren't\", \"hadn't\", 'will', 'should', 't', 'them', 'against', 'why', 'between', 'when', 'once', 'how', 'an', \"couldn't\", 'after', 'has', 'haven', 'i', 'ain', 'they', 'until', \"doesn't\", 'herself', 'on', 'its', 'aren', \"aren't\", 'll', 'theirs', 'she', 'did', \"you've\", 'each', 'doesn', 'as', \"needn't\", 'under', 'the', 'doing', 'there', 'mightn', 'up', 'was', 'didn', 'and', \"didn't\", 'who', \"haven't\", 'you', 'don', 'but', \"hasn't\", \"mustn't\", 'myself', 'his', 'at', 'out', \"wouldn't\", 'am', 've', 'a', 'during', 'is', 'y', 'it', 'too', 'themselves', 'about', 'had', 'other', 'shouldn', 'any', 'same', 'mustn', 'again', 'our', 'for', 'by', 'most', 's', \"don't\", 'no', \"won't\", 'having', 'all', 'can', 'yours', 'before', 'hers', 'both', 'himself', 'because', 'm', 'wasn', 'some', \"shouldn't\", 'him', 'yourself', 'shan', 'nor', 'few', 'where', 'd', 'do', 'just', 'above', 'needn', 'o', 'wouldn'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = [\"Hello! I am a good boy. My name is Montu Sharma.\", \"I am a MachineLearning Engineer.\"]\n",
    "corpus = [extracted_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_corpus(corpus, stop_words):   \n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words = stop_words\n",
    "\n",
    "    processed_corpus = []\n",
    "\n",
    "    for sentence in corpus:\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        # Remove stop words\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "        # Prepend [CLS] and append [SEP]\n",
    "        special_tokens = ['[CLS]'] + filtered_tokens + ['[SEP]']  \n",
    "        # Encode the special tokens\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(special_tokens)        \n",
    "\n",
    "        input_dict = {\n",
    "            'input_ids': token_ids,\n",
    "            'attention_mask': [1]*len(token_ids)\n",
    "        }        \n",
    "\n",
    "        # Set maximum length\n",
    "        padded_sentence = tokenizer.pad(input_dict, padding='max_length', max_length=30)        \n",
    "\n",
    "        attention_mask = [1 if i!= tokenizer.pad_token_id else 0 for i in padded_sentence['input_ids']]        \n",
    "        \n",
    "        # Append the padded sentence and its attention mask to processed_corpus\n",
    "        processed_corpus.append((padded_sentence['input_ids'], attention_mask))  \n",
    "\n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens: ['[CLS]', 'son', '##i', 'j', '##ha', 'business', 'analyst', '(', ')', '+', '91', '99', '##6', '##7', '##17', '##9', '##80', '##0', 'mumbai', '(', 'maharashtra', ')', 'em', '##ai', 'l', ':', 'son', '##ij', '##6', '##19', '##2', '@', 'gma', '##il', '.', 'com', 'profile', '•', 'techno', 'financial', 'management', 'professional', 'experience', '1', '0', '+', 'years', 'business', '/', 'system', 'analysis', ',', 'regulatory', 'reporting', ',', 'project', 'management', ',', 'requirement', 'gathering', ',', 'preparing', 'use', 'case', ',', 'test', 'case', ',', 'def', '##er', '##ral', 'list', ',', 'br', '##d', 'fr', '##d', '.', 'extensive', 'experience', 'client', 'facing', 'requirements', 'gathering', 'capture', 'process', 'map', 'process', 'vi', 'si', '##on', '.', 'def', '##t', 'design', 'review', 'various', 'documents', 'like', 'br', '##d', ',', 'fr', '##d', ',', 'use', 'case', 'specifications', ',', 'user', 'stories', ',', 'product', 'back', '##log', ',', 'sprint', 'back', '##log', 'rt', '##m', 'testing', 'documents', '.', 'pro', '##active', 'implementation', 'sd', '##lc', ',', 'agile', 'method', '##ologies', ',', 'story', 'boards', ',', 'sprint', 'cycles', ',', 'cha', 'ng', '##e', 'process', 'management', '.', 'good', 'knowledge', 'agile', 'methodology', 'includes', 'sprint', 'planning', ',', 'daily', 'sc', '##rum', 'calls', ',', 'assign', '##ing', 'priorities', 'user', 'stories', '.', 'involved', 'investigation', 'preparing', 'sharing', 'daily', '/', 'weekly', 'test', 'report', '(', 'w', '##sr', '/', 'ts', '##r', '&', 'ds', '##r', ')', ',', 'hs', '##d', 'anal', '##y', 'sis', '/', 'defect', 'density', '/', 'defect', 'leak', '##age', '.', 'education', 'degree', '|', 'date', 'earned', '|', 'mca', '|', '2010', '|', 'rajasthan', 'technical', 'university', 'bc', '##a', '|', '2007', '|', 'rajasthan', 'vi', '##dya', '##pee', '##th', 'university', 'professional', 'certification', '##s', 'ist', '##q', '##b', 'foundation', 'level', 'certification', '##s', 'ist', '##q', '##b', 'agile', 'test', '##e', 'r', 'certification', '##s', 'cs', '##m', '(', 'ce', '##rti', '##fi', '##e', 'sc', '##rum', 'master', ')', 'award', '/', 'achievement', 'go', 'mx', '-', 'award', 'excellence', '-', 'individual', '(', 'lt', '##i', ')', 'start', 'quarterly', 'performance', '(', 'tc', '##s', ')', 'page', '2', 'skills', '&', 'abilities', 'domain', 'capital', 'market', ',', 'banking', ',', 'cards', ',', 'payments', ',', 'ky', '##c', ',', '(', 'client', '##bo', '##ar', 'ding', ')', ',', 'regular', '##ity', 'reporting', '.', 'investment', 'ba', 'nk', '##ing', ',', 'trading', 'pr', '##ag', '##ram', '##ming', 'language', 'sql', ',', 'unix', 'business', '/', 'system', 'anal', '##sis', 'usage', ':', 'preparing', 'br', '##d', ',', 'fr', '##d', ',', 'use', 'case', ',', 'def', '##er', '##ral', 'list', 'test', 'cases', '.', 'analyzing', 'data', '’', 'received', 'feeds', ',', 'capacity', 'planning', 'est', '##ima', '##t', 'ions', ',', 'sc', '##rum', '&', 'agile', 'knowledge', 'software', ':', 'rules', 'harmony', ',', 'unix', 'testing', 'usage', ':', 'sit', '/', 'ua', '##t', 'testing', ',', 'defect', 'analysis', '##loading', 'test', 'results', 'software', ':', 'al', '##m', ',', 'ji', '##ra', ',', 'con', '##fl', '##uen', '##c', 'e', ',', 'red', '##mine', 'risk', 'management', 'functions', 'usage', ':', 'tracking', 'margin', 'level', 'clients', ',', 'ca', 'lc', '##ulating', 'monitoring', 'risk', 'projects', 'software', 'development', 'usage', ':', 'preparing', 'applications', 'software', ':', 'visual', 'studio', ',', 'sql', 'er', '##ver', ',', 'sas', ',', 'unix', 'experience', 'business', 'analyst', '|', 'lt', '##i', '|', 'dec', '2019', 'present', 'position', ':', 'business', 'analyst', 'location', ':', 'mumbai', 'client', ':', 'ci', '##ti', 'bank', '-', 'em', '##ea', 'company', ':', 'lt', '##i', 'pro', '##je', '##c', 'name', 'g', '##ft', '##s', 'reporting', 'analytic', 'team', 'size', '25', 'start', 'date', 'dec', '2019', 'end', 'date', 'present', 'project', 'description', 'support', 'functions', 'em', '##ea', 'regions', 'regulatory', 'reporting', 'projects', '.', 'commented', '[', 'n', '##1', ']', ':', 'page', '3', 'role', 'contribution', 'work', 'closely', 'w', '##h', 'business', 'users', 'system', 'developers', 'define', 'data', 'sources', 'detailed', 'business', 'rules', 'p', '&', 'l', ',', 'accounting', 'regulatory', 'reporting', 'systems', '.', 'assisted', '##sit', '##e', 'change', 'management', 'team', 'define', 'accounting', 'sc', '##hema', 'various', 'banking', 'products', '.', 'reviewed', 'manipulated', 'trade', 'market', 'data', 'stand', 'variety', 'scenarios', 'need', 'cater', '##ed', 'prepare', 'test', 'packs', '.', 'established', 'maintained', 'effective', 'communication', 'engagement', 'model', 'key', 'contacts', 'em', '##ea', 'regulatory', 'reporting', ',', 'te', 'ch', '##nology', 'line', 'business', 'teams', '.', 'con', '##tr', 'ib', '##uted', 'project', 'scope', ',', 'approach', 'timeline', '##s', ',', 'ensuring', 'em', '##ea', 'regulatory', 'impacts', 'understood', 'requirements', 'fully', 'met', '.', 'ensured', 'business', 'requirements', 'testing', 'strategies', 'clearly', 'documented', ',', 'communicated', ',', 'delivered', 'cr', '##it', 'ic', '##al', 'success', 'criteria', '.', 'ensured', 'test', 'environment', 'requirements', 'clearly', 'articulated', 'agreed', '.', 'ensured', 'issues', 'raised', '/', 'escalated', 'driven', 'resolution', '.', 'ensured', 'risks', '&', 'depend', '##encies', 'high', '##li', '##g', 'h', '##ted', ',', 'mit', '##iga', '##tion', 'options', 'presented', ',', 'project', 'management', 'em', '##ea', 'regulatory', 'reporting', 'team', '.', 'tools', 'unix', ',', 'reg', '##ins', '##ight', ',', 'rules', 'engine', ',', 'ji', '##ra', ',', 'confluence', ',', 'hp', '##q', '##c', ',', 'sql', 'developer', 'position', ':', 'sr', '.', 'functional', 'analyst', 'location', ':', 'mumbai', 'client', ':', 'bn', '##p', 'par', '##iba', '##s', 'company', ':', 'col', '##lab', '##era', 'technologies', 'project', 'n', '##e', 'connected', 'risk', 'team', 'size', '20', 'start', 'date', 'oct', '2018', 'end', 'date', 'oct', '2019', 'project', 'description', 'mao', '##s', ',', 'g', '##cars', 'role', 'contribution', 'role', 'required', 'analyzing', 'testing', 'connected', 'risk', 'online', 'portal', '.', 'requirement', 'analysis', ':', 'analyzing', 'require', '##me', 'nt', '##s', 'communicating', 'ba', 'better', 'understanding', 'involved', 'functional', ',', 'regression', 'ua', '##t', 'testing', '.', 'identification', 'page', '4', 'def', 'ec', '##t', ',', 'reporting', '##shore', 'team', '-', 'testing', 'fixed', 'involved', 'investigation', 'preparing', 'sharing', 'daily', '/', 'weekly', 'test', 'report', '(', 'w', '##sr', '/', 'ts', '##r', '&', 'ds', '##r', ')', 'ra', '##ci', 'defect', 'density', '&', 'management', '(', 'hs', '##d', 'anal', '##y', 'sis', '/', 'defect', 'density', '/', 'defect', 'leak', '##age', ')', '.', 'involved', 'preparation', 'test', 'plan', ',', 'test', 'summary', 'report', 'al', '##m', 'reports', 'tools', 'ji', '##ra', ',', 'al', '##m', ',', 'r', '##pa', 'position', ':', 'ba', '/', 'functional', 'test', '##er', 'location', ':', 'mumbai', 'client', ':', 'clear', 'corporation', 'india', '(', 'cc', '##il', ')', 'company', ':', 'tc', '##s', 'project', 'name', 'fore', '##x', 'clear', '&', 'swap', 'team', 'si', '##z', 'e', '15', 'start', 'date', 'aug', '2016', 'end', 'date', 'sep', '2018', 'project', 'description', 'ba', '–', 'ua', '##t', 'test', '##er', 'role', 'contribution', 'role', 'required', 'client', 'facing', 'requirements', 'gathering', 'capture', 'process', 'map', 'process', 'vision', '.', 'involved', 'identifying', 'automation', 'block', '##e', 'rs', 'identifying', 'automation', 'enable', '##rs', 'art', '##ic', '##ulating', 'client', '.', 'defined', 'set', 'q', '##a', 'strategy', '&', 'processes', '.', 'driven', '&', 'coordinated', 'risk', 'vertical', '##s', ',', 'post', '-', 'merger', 'cc', '##il', '.', 'plan', 'manage', '##lea', '##s', 'es', 'three', 'applications', '.', 'bottom', '-', 'line', 'res', 'po', '##ns', '##ibility', 'q', '##a', 'deliver', '##able', '##s', 'across', 'planning', '&', 'execution', 'phases', '.', 'carrying', 'requirement', 'analysis', 'new', 'enhancement', '##s', '&', 'changes', '.', 'preparation', 'test', 'plan', '/', 'test', 'strategy', 'every', 'release', '.', 'requirement', 'col', '##le', '##c', 'ti', '##on', '&', 'review', 'deliver', '##able', '##s', 'viz', '.', 'test', 'ap', 'pro', '##ach', ',', 'test', 'cases', 'etc', '.', 'define', 'testing', 'scope', ',', 'test', 'methodology', '&', 'strategy', 'release', '.', 'manage', 'quality', 'centre', ',', 'red', '##mine', '&', 'al', '##m', 'ensure', 'effective', 'usage', 'multiple', 'test', 'environments', '.', 'provide', 'directions', 'mentor', 'est', '##ing', 'team', 'members', 'working', 'different', 'projects', 'multiple', 'geo', '##graph', '##ies', '.', 'implemented', 'best', 'practice', 'helps', 'calculating', 'productivity', '&', 'used', 'substitute', 'test', 'plan', '.', 'involved', 'st', '##ru', '##cturing', 'shore', 'model', 'effective', 'co', '##ord', 'ina', '##tion', 'vendor', 'partners', '.', 'tools', 'ms', 'office', ',', 'red', '##mine', ',', 'al', '##m', 'position', ':', 'software', 'test', 'engineer', 'location', ':', 'sydney', ',', 'india', 'page', '5', 'company', ':', 'guru', '##s', 'software', 'project', 'name', '##ps', '##a', ',', 'simply', 'recruit', 'team', 'size', '15', 'start', 'date', 'sep', '2014', 'end', 'date', 'aug', '2016', 'project', 'de', 'script', '##ion', 'project', 'covers', 'web', 'ipad', 'application', 'online', 'survey', 'companies', 'creating', 'assessment', 'survey', '.', 'system', 'used', 'company', 'ad', '##min', '##ps', '##a', 'sales', 'person', '.', 'role', 'contribution', 'interface', '##d', 'directly', 'client', 'n', 'daily', 'basis', '.', 'ran', 'workshops', 'client', '##s', 'understand', 'end', 'end', 'process', 'lia', '##ise', 'various', 'front', 'office', 'teams', 'manage', 'testing', 'phases', 'system', 'testing', 'ua', '##t', 'drive', 'defect', 'def', '##er', '##ral', 'process', '.', 'involved', 'managing', 'process', 'f', 'overall', 'q', '##a', 'team', '&', 'amp', ';', 'thus', 'help', 'rai', '##s', 'ing', 'project', '.', 'providing', 'application', 'knowledge', 'team', 'members', '.', '.', 'provide', 'outputs', 'weekly', 'due', 'dil', '##igen', '##ce', 'updates', 'client', '.', 'understood', 'risks', ',', 'assumptions', ',', 'depend', '##encies', 'associated', 'pro', 'ce', '##ss', '.', 'worked', 'closely', 'development', 'team', 'ensure', 'requirements', 'accurately', 'mapped', 'suitable', 'development', '.', 'tools', 'red', '##mine', 'position', ':', 'application', '–', 'jr', '.', 'test', '##er', 'location', ':', 'mumbai', 'company', ':', 'shaw', '##man', 'software', 'project', 'name', 'de', '##bit', 'card', 'system', '(', 'dc', '##s', ')', ',', 'point', 'sale', '(', 'po', '##s', ')', 'team', 'size', '12', 'start', 'date', 'oct', '2013', 'end', 'date', 'sep', '2014', 'project', 'description', 'shaw', '##man', 'dc', '##s', ',', 'po', '##s', 'state', '-', '-', '-', 'art', 'plug', 'play', 'software', 'supporting', 'multiple', 'styles', 'operations', 'ranging', 'small', 'fast', 'food', 'outlet', ',', 'coffee', 'shops', 'role', 'contribution', 'perform', 'type', 'smoke', ',', 'functional', ',', 'integration', ',', 'system', ',', 'regression', 'testing', '.', 'lia', '##ise', 'various', 'front', 'office', 'teams', 'manage', 'testing', 'phases', 'system', 'testing', 'ua', '##t', 'drive', 'defect', 'def', '##e', 'rr', '##al', 'process', '.', 'writing', 'test', 'cases', 'executing', 'tools', 'home', 'tool', 'sql', '##m', '[SEP]'], \n",
      "Token IDs: [101, 2365, 2072, 1046, 3270, 2449, 12941, 1006, 1007, 1009, 6205, 5585, 2575, 2581, 16576, 2683, 17914, 2692, 8955, 1006, 12434, 1007, 7861, 4886, 1048, 1024, 2365, 28418, 2575, 16147, 2475, 1030, 20917, 4014, 1012, 4012, 6337, 1528, 21416, 3361, 2968, 2658, 3325, 1015, 1014, 1009, 2086, 2449, 1013, 2291, 4106, 1010, 10738, 7316, 1010, 2622, 2968, 1010, 9095, 7215, 1010, 8225, 2224, 2553, 1010, 3231, 2553, 1010, 13366, 2121, 7941, 2862, 1010, 7987, 2094, 10424, 2094, 1012, 4866, 3325, 7396, 5307, 5918, 7215, 5425, 2832, 4949, 2832, 6819, 9033, 2239, 1012, 13366, 2102, 2640, 3319, 2536, 5491, 2066, 7987, 2094, 1010, 10424, 2094, 1010, 2224, 2553, 15480, 1010, 5310, 3441, 1010, 4031, 2067, 21197, 1010, 9043, 2067, 21197, 19387, 2213, 5604, 5491, 1012, 4013, 19620, 7375, 17371, 15472, 1010, 29003, 4118, 20792, 1010, 2466, 7923, 1010, 9043, 12709, 1010, 15775, 12835, 2063, 2832, 2968, 1012, 2204, 3716, 29003, 16134, 2950, 9043, 4041, 1010, 3679, 8040, 6824, 4455, 1010, 23911, 2075, 18402, 5310, 3441, 1012, 2920, 4812, 8225, 6631, 3679, 1013, 4882, 3231, 3189, 1006, 1059, 21338, 1013, 24529, 2099, 1004, 16233, 2099, 1007, 1010, 26236, 2094, 20302, 2100, 24761, 1013, 21262, 4304, 1013, 21262, 17271, 4270, 1012, 2495, 3014, 1064, 3058, 3687, 1064, 22432, 1064, 2230, 1064, 16815, 4087, 2118, 4647, 2050, 1064, 2289, 1064, 16815, 6819, 25838, 28084, 2705, 2118, 2658, 10618, 2015, 21541, 4160, 2497, 3192, 2504, 10618, 2015, 21541, 4160, 2497, 29003, 3231, 2063, 1054, 10618, 2015, 20116, 2213, 1006, 8292, 28228, 8873, 2063, 8040, 6824, 3040, 1007, 2400, 1013, 6344, 2175, 25630, 1011, 2400, 8012, 1011, 3265, 1006, 8318, 2072, 1007, 2707, 12174, 2836, 1006, 22975, 2015, 1007, 3931, 1016, 4813, 1004, 7590, 5884, 3007, 3006, 1010, 8169, 1010, 5329, 1010, 10504, 1010, 18712, 2278, 1010, 1006, 7396, 5092, 2906, 22033, 1007, 1010, 3180, 3012, 7316, 1012, 5211, 8670, 25930, 2075, 1010, 6202, 10975, 8490, 6444, 6562, 2653, 29296, 1010, 19998, 2449, 1013, 2291, 20302, 6190, 8192, 1024, 8225, 7987, 2094, 1010, 10424, 2094, 1010, 2224, 2553, 1010, 13366, 2121, 7941, 2862, 3231, 3572, 1012, 20253, 2951, 1521, 2363, 14172, 1010, 3977, 4041, 9765, 9581, 2102, 15956, 1010, 8040, 6824, 1004, 29003, 3716, 4007, 1024, 3513, 9396, 1010, 19998, 5604, 8192, 1024, 4133, 1013, 25423, 2102, 5604, 1010, 21262, 4106, 18570, 3231, 3463, 4007, 1024, 2632, 2213, 1010, 10147, 2527, 1010, 9530, 10258, 24997, 2278, 1041, 1010, 2417, 11233, 3891, 2968, 4972, 8192, 1024, 9651, 7785, 2504, 7846, 1010, 6187, 29215, 10924, 8822, 3891, 3934, 4007, 2458, 8192, 1024, 8225, 5097, 4007, 1024, 5107, 2996, 1010, 29296, 9413, 6299, 1010, 21871, 1010, 19998, 3325, 2449, 12941, 1064, 8318, 2072, 1064, 11703, 10476, 2556, 2597, 1024, 2449, 12941, 3295, 1024, 8955, 7396, 1024, 25022, 3775, 2924, 1011, 7861, 5243, 2194, 1024, 8318, 2072, 4013, 6460, 2278, 2171, 1043, 6199, 2015, 7316, 23521, 2136, 2946, 2423, 2707, 3058, 11703, 10476, 2203, 3058, 2556, 2622, 6412, 2490, 4972, 7861, 5243, 4655, 10738, 7316, 3934, 1012, 7034, 1031, 1050, 2487, 1033, 1024, 3931, 1017, 2535, 6691, 2147, 4876, 1059, 2232, 2449, 5198, 2291, 9797, 9375, 2951, 4216, 6851, 2449, 3513, 1052, 1004, 1048, 1010, 9529, 10738, 7316, 3001, 1012, 7197, 28032, 2063, 2689, 2968, 2136, 9375, 9529, 8040, 28433, 2536, 8169, 3688, 1012, 8182, 20063, 3119, 3006, 2951, 3233, 3528, 16820, 2342, 23488, 2098, 7374, 3231, 15173, 1012, 2511, 5224, 4621, 4807, 8147, 2944, 3145, 10402, 7861, 5243, 10738, 7316, 1010, 8915, 10381, 21020, 2240, 2449, 2780, 1012, 9530, 16344, 21307, 12926, 2622, 9531, 1010, 3921, 17060, 2015, 1010, 12725, 7861, 5243, 10738, 14670, 5319, 5918, 3929, 2777, 1012, 16316, 2449, 5918, 5604, 9942, 4415, 8832, 1010, 24162, 1010, 5359, 13675, 4183, 24582, 2389, 3112, 9181, 1012, 16316, 3231, 4044, 5918, 4415, 20742, 3530, 1012, 16316, 3314, 2992, 1013, 26814, 5533, 5813, 1012, 16316, 10831, 1004, 12530, 15266, 2152, 3669, 2290, 1044, 3064, 1010, 10210, 13340, 3508, 7047, 3591, 1010, 2622, 2968, 7861, 5243, 10738, 7316, 2136, 1012, 5906, 19998, 1010, 19723, 7076, 18743, 1010, 3513, 3194, 1010, 10147, 2527, 1010, 13693, 1010, 6522, 4160, 2278, 1010, 29296, 9722, 2597, 1024, 5034, 1012, 8360, 12941, 3295, 1024, 8955, 7396, 1024, 24869, 2361, 11968, 18410, 2015, 2194, 1024, 8902, 20470, 6906, 6786, 2622, 1050, 2063, 4198, 3891, 2136, 2946, 2322, 2707, 3058, 13323, 2760, 2203, 3058, 13323, 10476, 2622, 6412, 15158, 2015, 1010, 1043, 20745, 2535, 6691, 2535, 3223, 20253, 5604, 4198, 3891, 3784, 9445, 1012, 9095, 4106, 1024, 20253, 5478, 4168, 23961, 2015, 20888, 8670, 2488, 4824, 2920, 8360, 1010, 26237, 25423, 2102, 5604, 1012, 8720, 3931, 1018, 13366, 14925, 2102, 1010, 7316, 19208, 2136, 1011, 5604, 4964, 2920, 4812, 8225, 6631, 3679, 1013, 4882, 3231, 3189, 1006, 1059, 21338, 1013, 24529, 2099, 1004, 16233, 2099, 1007, 10958, 6895, 21262, 4304, 1004, 2968, 1006, 26236, 2094, 20302, 2100, 24761, 1013, 21262, 4304, 1013, 21262, 17271, 4270, 1007, 1012, 2920, 7547, 3231, 2933, 1010, 3231, 12654, 3189, 2632, 2213, 4311, 5906, 10147, 2527, 1010, 2632, 2213, 1010, 1054, 4502, 2597, 1024, 8670, 1013, 8360, 3231, 2121, 3295, 1024, 8955, 7396, 1024, 3154, 3840, 2634, 1006, 10507, 4014, 1007, 2194, 1024, 22975, 2015, 2622, 2171, 18921, 2595, 3154, 1004, 19948, 2136, 9033, 2480, 1041, 2321, 2707, 3058, 15476, 2355, 2203, 3058, 19802, 2760, 2622, 6412, 8670, 1516, 25423, 2102, 3231, 2121, 2535, 6691, 2535, 3223, 7396, 5307, 5918, 7215, 5425, 2832, 4949, 2832, 4432, 1012, 2920, 12151, 19309, 3796, 2063, 12667, 12151, 19309, 9585, 2869, 2396, 2594, 10924, 7396, 1012, 4225, 2275, 1053, 2050, 5656, 1004, 6194, 1012, 5533, 1004, 14206, 3891, 7471, 2015, 1010, 2695, 1011, 7660, 10507, 4014, 1012, 2933, 6133, 19738, 2015, 9686, 2093, 5097, 1012, 3953, 1011, 2240, 24501, 13433, 3619, 13464, 1053, 2050, 8116, 3085, 2015, 2408, 4041, 1004, 7781, 12335, 1012, 4755, 9095, 4106, 2047, 22415, 2015, 1004, 3431, 1012, 7547, 3231, 2933, 1013, 3231, 5656, 2296, 2713, 1012, 9095, 8902, 2571, 2278, 14841, 2239, 1004, 3319, 8116, 3085, 2015, 26619, 1012, 3231, 9706, 4013, 6776, 1010, 3231, 3572, 4385, 1012, 9375, 5604, 9531, 1010, 3231, 16134, 1004, 5656, 2713, 1012, 6133, 3737, 2803, 1010, 2417, 11233, 1004, 2632, 2213, 5676, 4621, 8192, 3674, 3231, 10058, 1012, 3073, 7826, 10779, 9765, 2075, 2136, 2372, 2551, 2367, 3934, 3674, 20248, 14413, 3111, 1012, 7528, 2190, 3218, 7126, 20177, 15836, 1004, 2109, 7681, 3231, 2933, 1012, 2920, 2358, 6820, 19159, 5370, 2944, 4621, 2522, 8551, 27118, 3508, 21431, 5826, 1012, 5906, 5796, 2436, 1010, 2417, 11233, 1010, 2632, 2213, 2597, 1024, 4007, 3231, 3992, 3295, 1024, 3994, 1010, 2634, 3931, 1019, 2194, 1024, 11972, 2015, 4007, 2622, 2171, 4523, 2050, 1010, 3432, 13024, 2136, 2946, 2321, 2707, 3058, 19802, 2297, 2203, 3058, 15476, 2355, 2622, 2139, 5896, 3258, 2622, 4472, 4773, 25249, 4646, 3784, 5002, 3316, 4526, 7667, 5002, 1012, 2291, 2109, 2194, 4748, 10020, 4523, 2050, 4341, 2711, 1012, 2535, 6691, 8278, 2094, 3495, 7396, 1050, 3679, 3978, 1012, 2743, 9656, 7396, 2015, 3305, 2203, 2203, 2832, 22393, 5562, 2536, 2392, 2436, 2780, 6133, 5604, 12335, 2291, 5604, 25423, 2102, 3298, 21262, 13366, 2121, 7941, 2832, 1012, 2920, 6605, 2832, 1042, 3452, 1053, 2050, 2136, 1004, 23713, 1025, 2947, 2393, 15547, 2015, 13749, 2622, 1012, 4346, 4646, 3716, 2136, 2372, 1012, 1012, 3073, 27852, 4882, 2349, 29454, 29206, 3401, 14409, 7396, 1012, 5319, 10831, 1010, 17568, 1010, 12530, 15266, 3378, 4013, 8292, 4757, 1012, 2499, 4876, 2458, 2136, 5676, 5918, 14125, 17715, 7218, 2458, 1012, 5906, 2417, 11233, 2597, 1024, 4646, 1516, 3781, 1012, 3231, 2121, 3295, 1024, 8955, 2194, 1024, 8233, 2386, 4007, 2622, 2171, 2139, 16313, 4003, 2291, 1006, 5887, 2015, 1007, 1010, 2391, 5096, 1006, 13433, 2015, 1007, 2136, 2946, 2260, 2707, 3058, 13323, 2286, 2203, 3058, 19802, 2297, 2622, 6412, 8233, 2386, 5887, 2015, 1010, 13433, 2015, 2110, 1011, 1011, 1011, 2396, 13354, 2377, 4007, 4637, 3674, 6782, 3136, 7478, 2235, 3435, 2833, 13307, 1010, 4157, 7340, 2535, 6691, 4685, 2828, 5610, 1010, 8360, 1010, 8346, 1010, 2291, 1010, 26237, 5604, 1012, 22393, 5562, 2536, 2392, 2436, 2780, 6133, 5604, 12335, 2291, 5604, 25423, 2102, 3298, 21262, 13366, 2063, 25269, 2389, 2832, 1012, 3015, 3231, 3572, 23448, 5906, 2188, 6994, 29296, 2213, 102], \n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "processed_corpus = process_corpus(corpus, stop_words)\n",
    "\n",
    "# Display\n",
    "for token_ids, attention_mask in processed_corpus:\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    \n",
    "    data = {\"tokens\": tokens, \n",
    "            \"token_ids\": token_ids, \n",
    "            \"attention_mask\": attention_mask}\n",
    "    \n",
    "    print(f\"\\nTokens: {tokens}, \\nToken IDs: {token_ids}, \\nAttention Mask: {attention_mask}\")\n",
    "    \n",
    "    df = pd.DataFrame(list(zip(tokens, token_ids, attention_mask)), columns=['Token', 'IDs', 'Attention Mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>IDs</th>\n",
       "      <th>Attention Mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>son</td>\n",
       "      <td>2365</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>##i</td>\n",
       "      <td>2072</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>j</td>\n",
       "      <td>1046</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>##ha</td>\n",
       "      <td>3270</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>home</td>\n",
       "      <td>2188</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>tool</td>\n",
       "      <td>6994</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>sql</td>\n",
       "      <td>29296</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>##m</td>\n",
       "      <td>2213</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>[SEP]</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1341 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Token    IDs  Attention Mask\n",
       "0     [CLS]    101               1\n",
       "1       son   2365               1\n",
       "2       ##i   2072               1\n",
       "3         j   1046               1\n",
       "4      ##ha   3270               1\n",
       "...     ...    ...             ...\n",
       "1336   home   2188               1\n",
       "1337   tool   6994               1\n",
       "1338    sql  29296               1\n",
       "1339    ##m   2213               1\n",
       "1340  [SEP]    102               1\n",
       "\n",
       "[1341 rows x 3 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "attention_mask = torch.tensor(attention_mask).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.4546,  0.4363, -0.3296,  ..., -0.3637,  0.4343, -0.0601],\n",
       "         [ 0.5521,  0.2751, -0.7303,  ..., -0.0442,  0.2307, -0.0526],\n",
       "         [ 0.6285,  0.1741,  0.2261,  ...,  0.1098,  0.1802, -1.2926],\n",
       "         ...,\n",
       "         [ 0.1009, -0.0152, -0.0759,  ..., -0.1650, -0.3457, -0.1263],\n",
       "         [ 0.0305, -0.2906,  0.0715,  ..., -0.2525, -0.3476,  0.1595],\n",
       "         [ 0.3015,  0.2462,  0.0686,  ..., -0.1453, -0.3376, -0.2336]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.9604, -0.6381, -0.9947,  0.9564,  0.9562, -0.3733,  0.9847,  0.4575,\n",
       "         -0.9756, -1.0000, -0.9169,  0.9966,  0.9572,  0.8458,  0.9297, -0.9705,\n",
       "         -0.8064, -0.7094,  0.5099, -0.8736,  0.8075,  1.0000, -0.5156,  0.6334,\n",
       "          0.7041,  0.9997, -0.9344,  0.9091,  0.9654,  0.8106, -0.8785,  0.5455,\n",
       "         -0.9869, -0.4690, -0.9929, -0.9956,  0.7480, -0.8572, -0.2594, -0.3871,\n",
       "         -0.8785,  0.6669,  1.0000,  0.3406,  0.7452, -0.5211, -1.0000,  0.4329,\n",
       "         -0.9145,  0.9956,  0.9943,  0.9866,  0.5012,  0.7786,  0.7281, -0.6781,\n",
       "          0.1955,  0.4304, -0.3674, -0.7627, -0.6861,  0.7091, -0.9868, -0.9372,\n",
       "          0.9963,  0.9521, -0.5205, -0.6020, -0.3920,  0.0997,  0.9742,  0.3425,\n",
       "         -0.4808, -0.8430,  0.9250,  0.4735, -0.8034,  1.0000, -0.8964, -0.9825,\n",
       "          0.9789,  0.9611,  0.7635, -0.8445,  0.8605, -1.0000,  0.8403, -0.4273,\n",
       "         -0.9911,  0.5164,  0.8422, -0.5265,  0.8748,  0.7988, -0.8482, -0.7957,\n",
       "         -0.6096, -0.9774, -0.6159, -0.7622,  0.1856, -0.4597, -0.6526, -0.5587,\n",
       "          0.5328, -0.7263, -0.7576,  0.7434,  0.6546,  0.7813,  0.5401, -0.5938,\n",
       "          0.7197, -0.9623,  0.8264, -0.5648, -0.9913, -0.8224, -0.9879,  0.8260,\n",
       "         -0.7771, -0.4668,  0.9665, -0.6657,  0.6435, -0.4587, -0.9931, -1.0000,\n",
       "         -0.9656, -0.8437, -0.4670, -0.4906, -0.9763, -0.9658,  0.7698,  0.9733,\n",
       "          0.4812,  1.0000, -0.6349,  0.9257, -0.8455, -0.9059,  0.9193, -0.6282,\n",
       "          0.9556,  0.8887, -0.8413,  0.5219, -0.8372,  0.6566, -0.9535, -0.4738,\n",
       "         -0.9672, -0.9454, -0.5301,  0.9678, -0.9040, -0.9959, -0.6421, -0.5590,\n",
       "         -0.7723,  0.8964,  0.9456,  0.6480, -0.5624,  0.7120,  0.8499,  0.7814,\n",
       "         -0.9037, -0.5350,  0.7626, -0.6548, -0.9841, -0.9767, -0.6850,  0.7010,\n",
       "          0.9907,  0.8738,  0.6035,  0.9773, -0.5754,  0.9430, -0.9657,  0.9832,\n",
       "         -0.5819,  0.4328, -0.7725,  0.4659, -0.9293,  0.4508,  0.9722, -0.9377,\n",
       "         -0.8854, -0.4364, -0.6009, -0.5471, -0.9735,  0.7311, -0.5639, -0.5842,\n",
       "         -0.5064,  0.9133,  0.9979,  0.9048,  0.7624,  0.9097, -0.9405, -0.6925,\n",
       "          0.3921,  0.4473,  0.4994,  0.9937, -0.8584, -0.5071, -0.9129, -0.9843,\n",
       "          0.3111, -0.9481, -0.4452, -0.8817,  0.8764, -0.7293,  0.8694,  0.6372,\n",
       "         -0.9984, -0.8499,  0.6262, -0.6612,  0.6551, -0.4538,  0.5632,  0.9959,\n",
       "         -0.8012,  0.9163,  0.9132, -0.9911, -0.8897,  0.9432, -0.5375,  0.9598,\n",
       "         -0.8660,  0.9981,  0.9957,  0.9390, -0.9200, -0.9294, -0.9839, -0.9473,\n",
       "         -0.3737,  0.6172,  0.9881,  0.8217,  0.6606, -0.4232, -0.8530,  0.9999,\n",
       "         -0.2698, -0.9713, -0.4190, -0.6426, -0.9864,  0.9808,  0.6444,  0.8800,\n",
       "         -0.7432, -0.9178, -0.9553,  0.9819,  0.4669,  0.9985, -0.6171, -0.9894,\n",
       "         -0.8264, -0.9311,  0.2524, -0.4464, -0.8378,  0.2922, -0.9738,  0.6848,\n",
       "          0.7294,  0.8140, -0.9511,  0.9999,  1.0000,  0.9813,  0.8983,  0.9562,\n",
       "         -1.0000, -0.4673,  1.0000, -0.9998, -1.0000, -0.9656, -0.9173,  0.6023,\n",
       "         -1.0000, -0.2757, -0.3691, -0.9048,  0.9518,  0.9534,  0.9992, -1.0000,\n",
       "          0.8882,  0.9635, -0.8576,  0.9979, -0.7701,  0.9481,  0.7303,  0.7873,\n",
       "         -0.4899,  0.6797, -0.9968, -0.9759, -0.9268, -0.9321,  1.0000,  0.5211,\n",
       "         -0.9267, -0.9193,  0.8374, -0.4174,  0.2740, -0.9711, -0.4815,  0.9175,\n",
       "          0.9362,  0.5111,  0.4808, -0.8659,  0.5602,  0.5408,  0.5513,  0.8170,\n",
       "         -0.8852, -0.8076, -0.9098,  0.3399, -0.9043, -0.9617,  0.9801, -0.6606,\n",
       "          0.9926,  1.0000,  0.5162, -0.9490,  0.8870,  0.6140, -0.8293,  1.0000,\n",
       "          0.9482, -0.9778, -0.7518,  0.8175, -0.7777, -0.8517,  0.9999, -0.4997,\n",
       "         -0.9567, -0.9221,  0.9857, -0.9884,  0.9999, -0.9496, -0.9679,  0.9653,\n",
       "          0.9276, -0.9278, -0.9016,  0.4149, -0.9340,  0.5712, -0.9780,  0.9552,\n",
       "          0.7370, -0.4018,  0.9171, -0.9875, -0.7947,  0.6651, -0.9441, -0.7620,\n",
       "          0.9926,  0.7215, -0.5106,  0.2041, -0.6441, -0.3472, -0.9716,  0.9127,\n",
       "          1.0000, -0.7486,  0.9697, -0.7607, -0.2832,  0.3384,  0.7399,  0.7971,\n",
       "         -0.6379, -0.8501,  0.9532, -0.9946, -0.9892,  0.9063,  0.5987, -0.4613,\n",
       "          1.0000,  0.8811,  0.5354,  0.6042,  0.9995,  0.2787,  0.7155,  0.9817,\n",
       "          0.9749, -0.4901,  0.7816,  0.9732, -0.9929, -0.5127, -0.8041,  0.2114,\n",
       "         -0.9487, -0.2473, -0.9690,  0.9786,  0.9964,  0.6216,  0.5487,  0.9759,\n",
       "          1.0000, -0.6349,  0.7331, -0.8731,  0.9602, -1.0000, -0.9345, -0.6147,\n",
       "         -0.4390, -0.9806, -0.5906,  0.4664, -0.9833,  0.9858,  0.8864, -0.9985,\n",
       "         -0.9878, -0.6119,  0.9892,  0.2742, -0.9993, -0.8551, -0.6360,  0.9378,\n",
       "         -0.6035, -0.9597, -0.7779, -0.6867,  0.6528, -0.5184,  0.7157,  0.9715,\n",
       "          0.2355, -0.9670, -0.7229, -0.3782, -0.9583,  0.9480, -0.9293, -0.9972,\n",
       "         -0.4733,  1.0000, -0.5296,  0.9861,  0.8719,  0.8673, -0.5459,  0.5075,\n",
       "          0.9903,  0.5164, -0.9369, -0.9887, -0.9527, -0.6694,  0.9022,  0.9246,\n",
       "          0.9350,  0.8994,  0.9186,  0.5471, -0.3178,  0.3741,  1.0000, -0.5014,\n",
       "         -0.4108, -0.7439, -0.5320, -0.5324, -0.7825,  1.0000,  0.6391,  0.8852,\n",
       "         -0.9866, -0.9904, -0.9763,  1.0000,  0.8929, -0.9286,  0.8607,  0.9229,\n",
       "         -0.4704,  0.9450, -0.5630, -0.5574,  0.4581,  0.4311,  0.9306, -0.8069,\n",
       "         -0.9645, -0.8309,  0.7539, -0.9606,  1.0000, -0.8293, -0.4789, -0.6832,\n",
       "         -0.7102,  0.9228,  0.3264, -0.9619, -0.5139,  0.5044,  0.9737,  0.5909,\n",
       "         -0.7721, -0.9492,  0.9898,  0.9745, -0.9917, -0.9224,  0.9544, -0.9775,\n",
       "          0.8379,  1.0000,  0.6728,  0.8863,  0.5913, -0.7748,  0.6259, -0.8290,\n",
       "          0.9158, -0.9668, -0.6417, -0.5427,  0.6692, -0.4683, -0.5729,  0.8253,\n",
       "          0.3498, -0.7640, -0.8267, -0.4588,  0.6746,  0.9592, -0.4401, -0.4751,\n",
       "          0.3630, -0.1821, -0.9581, -0.6164, -0.6682, -1.0000,  0.8803, -1.0000,\n",
       "          0.8903,  0.6728, -0.3259,  0.9130,  0.5721,  0.8831, -0.8292, -0.9825,\n",
       "         -0.4595,  0.8141, -0.6484, -0.9225, -0.8424,  0.6675, -0.5015,  0.4751,\n",
       "         -0.9003,  0.8137, -0.5220,  1.0000,  0.4682, -0.9296, -0.9967,  0.5420,\n",
       "         -0.5623,  1.0000, -0.9771, -0.9569,  0.5286, -0.9458, -0.8815,  0.6116,\n",
       "          0.2166, -0.9546, -0.9971,  0.9614,  0.9870, -0.6960,  0.7222, -0.6313,\n",
       "         -0.8926,  0.2801,  0.9952,  0.9866,  0.7746,  0.9652,  0.4757, -0.3847,\n",
       "          0.9565,  0.3807,  0.8771,  0.5206,  1.0000,  0.6652, -0.9533, -0.5960,\n",
       "         -0.9896, -0.5014, -0.9708,  0.5905,  0.5537,  0.8902, -0.5857,  0.9623,\n",
       "         -0.9815,  0.2665, -0.9396, -0.9459,  0.5695, -0.9589, -0.9841, -0.9775,\n",
       "          0.8918, -0.6911, -0.1992,  0.3708,  0.2027,  0.7508,  0.6646, -1.0000,\n",
       "          0.9432,  0.7006,  0.9881,  0.9719,  0.8280,  0.8396,  0.6725, -0.9854,\n",
       "         -0.9969, -0.6203, -0.5270,  0.8835,  0.8748,  0.8868,  0.7373, -0.6377,\n",
       "         -0.7029, -0.9211, -0.4820, -0.9885,  0.7424, -0.9442, -0.9922,  0.9607,\n",
       "          0.2611, -0.3925, -0.5061, -0.9658,  0.9930,  0.9242,  0.6735,  0.3827,\n",
       "          0.6394,  0.9205,  0.9880,  0.9791, -0.9813,  0.9501, -0.9373,  0.6407,\n",
       "          0.7033, -0.9577,  0.4690,  0.7271, -0.7619,  0.4846, -0.6178, -0.9883,\n",
       "          0.8742, -0.3423,  0.8566, -0.6644, -0.2784, -0.7250, -0.5336, -0.9153,\n",
       "         -0.9042,  0.7916,  0.7513,  0.9407,  0.9636, -0.3599, -0.9437, -0.5193,\n",
       "         -0.9693, -0.9454,  0.9810, -0.4277, -0.7606,  0.9729,  0.4452,  0.8627,\n",
       "          0.7262, -0.6675, -0.5773, -0.9007,  0.8994, -0.8060, -0.8493, -0.8687,\n",
       "          0.8869,  0.6029,  1.0000, -0.9738, -0.9934, -0.6709, -0.7070,  0.6354,\n",
       "         -0.8819, -1.0000,  0.5829, -0.8983,  0.9297, -0.9670,  0.9795, -0.9688,\n",
       "         -0.9949, -0.6487,  0.6871,  0.9530, -0.7584, -0.9472,  0.7427, -0.8110,\n",
       "          0.9988,  0.8907, -0.9228, -0.5067,  0.7726, -0.9876, -0.7801,  0.9519]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(token_ids, attention_mask = attention_mask)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter stop words and encode each sentence separately\n",
    "# encoded_corpus = []\n",
    "# for w in corpus:\n",
    "#     # Tokenize the sentence\n",
    "#     tokens = tokenizer.tokenize(w)\n",
    "    \n",
    "#     # Filter out stop words\n",
    "#     filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "#     # Encode the filtered tokens\n",
    "#     encoded_sentence = tokenizer.convert_tokens_to_ids(filtered_tokens)\n",
    "#     encoded_corpus.append(encoded_sentence)\n",
    "# encoded_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = tokenizer.encode(corpus, max_length=15, padding='max_length')\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = tokenizer.convert_ids_to_tokens(encoded_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'hello', '!', 'good', 'boy', '.', 'name', 'mont', '##u', 'sharma', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtered_tokens = ['[CLS]'] + filtered_tokens + ['[SEP]']\n",
    "# print(filtered_tokens)\n",
    "# len(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtered_tokens = filtered_tokens + ['[PAD]']\n",
    "# len(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention_mask = [1 if i != '[PAD]' else 0 for i in filtered_tokens]\n",
    "# attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(list(zip(filtered_tokens, token_ids)), columns=['Token', 'IDs'])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
