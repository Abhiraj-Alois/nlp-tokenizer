{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PdfParser:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "\n",
    "    def extract_text(self) -> str:\n",
    "        reader = PdfReader(self.filepath) \n",
    "        text = ''.join(page.extract_text() for page in reader.pages)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      "Naveen  Challa                                                   \n",
      "Email : Naveen.objects @gmail.com  \n",
      "Mobile:  +91-9032993324  \n",
      "                                                                                                                 \n",
      "              \n",
      " PROFESSIONAL SUMMARY  \n",
      " \n",
      "• Having 5+ years  of experience in SQL Server development, T -SQL, MSBI , SSIS , SSRS , SSAS , Data warehouse Designing, Data \n",
      "Analysis etc.  \n",
      "• Having good understanding of Banking, Finance and Trading domain  \n",
      "• Experience in Query Optimization and performance tuning of T -SQL queries, stored Procedure  and functions  \n",
      "• Experience in Application support and Databas e development and Technical Code Review  \n",
      "• Highly analytical candidate with extensive and progressive experience in design, develop, support and management of \n",
      "application system.  \n",
      "• Extensive knowledge on Reporting using complex SQL queries, stored procedures, functions, packages,   triggers, Indexing, \n",
      "Join, Views, and transactions in T -SQL \n",
      "• Have Strong US General Insurance, USA Define Benefit, Law and Finance Domain knowledge  \n",
      "• Implement enhancement/bug fixing/unit testing within pre -set deadlines with  high quality  \n",
      "• Experience in Data analysis, Data Cleansing, development, Implementation, Reporting and documentation projects  \n",
      "• Providing post deployment support for upgrades/changes/enhancements done on UAT & Production Environments.  \n",
      "• Designed  and developed the Power BI Reports  & Dashboards  using SQL Server, SSAS Tabular and Excel Sources.  \n",
      "• Created rich visualized Power BI reports  (Used Pie charts, Line charts and Tree Maps, Bar & Column charts, Tabular, Matrix, \n",
      "Cards, Slicers, Gauge, KPI’s and Custom Visualiz ations .     \n",
      "• Comfortable in working with Filters/Calculated Columns/Measures/Relationships and Transformations of Edit Query  section.   \n",
      "• Published Power  BI pbix  files from  Power  bi desktop to  Power  bi service .   \n",
      "• Shared the reports by creating a Content Pack  in app.powerbi.com.  \n",
      "• Worked on On-Premises Gateway  to refresh the data sources.  \n",
      "• Created new calculated columns and calculated measures using DAX Expressions . \n",
      "• Getting the data from On-Premises data source  bases to perform visualizations using Power Bi Desktop.  \n",
      "• Created rich visualized  power bi  reports  and dashboards  using KPI.  \n",
      "• Created charts like Pies, Lines, Tree Map, Area, Scatter, Table Heat Maps, Combo, Tabular, Matrix, Funnel, Single Card, Multi  \n",
      "row card, Gau ge and Slicer etc.  \n",
      "• Imported Custom Visuals from Microsoft Power Bi gallery as per client request.  \n",
      "• Comfortable in working with Filters/Calculated Columns/Measures/Relationships and Transformations of Edit Query  section.  \n",
      "• Implemented  Row Level Security  (RLS) as part of security.  \n",
      "• Published the reports into Power BI Service  & shared the dashboards to the users.  \n",
      "• Worked on On-Premises Gateway  to refresh the data sources in Power Bi Service.  \n",
      "• Created Content packs  that package up a dashboard, report and dataset . \n",
      "• Performed required calculations using DAX Expressions  as per busine ss needs (Table, String, Date and Logical functions).  \n",
      "• Experience in optimizing various database objects and T -SQL queries using appropriate joins and indexes.  \n",
      "• Monitor, Optimize and Tune complex T SQL scripts and Stored Procedures based on the execution pla ns \n",
      " \n",
      " \n",
      " \n",
      "PROFESSIONAL EXPERIENCE:  \n",
      "         \n",
      "• Working as Power BI /MSBI Developer for Citius Tech from Dec 2022 to Oct 2023  \n",
      "• Work ed as MBSI Developer for Fujitsu from Dec 2019 to June 2022  \n",
      "• Worked as Software Engineer for Oorja Services from Dec 2018 – Nov 2019  \n",
      "  \n",
      " \n",
      "CORE COMPETENCE  \n",
      " \n",
      "ETL Tool                    :  SQL Server Integration Services (SSIS).  \n",
      "Languages                 :  C, T-SQL & DAX (Data Analysis Expressions).  \n",
      "OLAP Tools    :  SSAS and SSAS Tabular.  Reporting Tools                    :    SSRS, MS Power BI (Desktop, Service  ,Pivot)  \n",
      "RDBMS                    :  MS SQL Server 2016 \\ 2012 \\2017 \\2019  \n",
      "Azure     :  Azure Storage, ADF, ADB  \n",
      "Power Platform.                                  :              Power Apps,Power Automation .Fabric \n",
      "Azure Platform.                                   :             Azure Blob, Azure Data lake,Azure Synapse,Azure Data factory, Azure Data bricks  \n",
      " \n",
      "PROJECT WORK:  \n",
      " \n",
      "Project Title    :  Saxo Bank  \n",
      "Tool/Language  :  SSIS, SSAS, SSRS, Power Pivot, Power Apps, Automate,Azure Devops Power  BI, ADF,Azure Synapse  \n",
      "Role                        :   ETL & Power BI Developer  \n",
      "          Duration                :   December  2022 to Oct 2023  \n",
      " \n",
      "Project Description:  \n",
      " \n",
      "Saxo Bank A/S is a fully licensed and regulated European bank specialization in online trading and investment across global financial \n",
      "markets. Saxo Bank enables private investors and institutional clients to trade FX, CFDs, ETFs, Stocks, Futures, Options a nd other \n",
      "derivatives via multi -award winning online trading platforms. In addition, Saxo Bank offers professional portfolio and fund \n",
      "management.  \n",
      " \n",
      "Roles & Responsibilities:  \n",
      " \n",
      "• Creating all types of SSRS reports (Drill down, Drill through, Parameterized, Snaps hot ,Cache)  \n",
      "• Creating Dash Boards using SSAS Cubes and Power BI standalone models for Business users to access the model and analyze \n",
      "the data.  \n",
      "• Created Pivot using Power Pivot, Geographical models using Power Maps and visualizations using Power View and Powe r \n",
      "Chart.  \n",
      "• Created Automated flows, Scheduled flows  \n",
      "• Created Instant flows and desktop flows for RPA  \n",
      "• Created Canvas apps & Model -driven apps  \n",
      "• Integrate with Microsoft Teams and Embed a canvas app in Teams  \n",
      "• Implement healthy ALM using solutions and Administer Po wer Apps  \n",
      "• Used Power Automate to Integrate With Teams,SharePoint  \n",
      "• Use flows with Dataverse and Trigger flows with Dataverse events  \n",
      "• Created so many dashboard reports, user parameters, slicers and other visualizations.  \n",
      "• Scheduling the Dashboards to refresh in P ower Bi Desktop.  \n",
      "• Used visualizations like Column Chart, Stacked chart, Pie chart, Slicers, Donut Charts, Row Cards, Multi Row card, Line Chart . \n",
      "• Sharing the reports with Business users with the help of Content packs.  \n",
      "• Scheduling E -mail Driven SSRS reports on  Weekly, Monthly basis.  \n",
      "• Developed Dashboard reports using Power view and Power Pivot.  \n",
      "• Created Pipelines using Azure Devops  \n",
      "• Used Azure Synapse for ETL Orchestration and Analytics  \n",
      "• Involved in Azure Devops  Authenticate access with personal access tokens  \n",
      "• Configured Azure Boards and Create your backlog  \n",
      "• Created Azure Devops Repos and Clone an existing Git repository  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Project Title         :  Drug File Pricing Response  \n",
      "Tool/Language        :   SSIS, SSRS, SSAS, Power BI Deskt op,Apps, Automate,Azure Synapse,ADF SQL Server 2014  \n",
      "Role                           : ETL,Datamodeling, Power BI Developer  \n",
      "Duration                   : Jul 2021 to June  2022 \n",
      " \n",
      "Project Description:  \n",
      " \n",
      "• Objective is to maintain the historical data generated from the nationwide network chain and to calculation, the revenue \n",
      "generated from the different chain of stores. Through its nationwide networks, the company delivers a number of leading healt h benefit solutions through a broad portfolio of integrated health care plans and related services, along with a wide range of \n",
      "specialty products such as life and disability insurance benefits, pharmacy benefit managements.      \n",
      " \n",
      "Roles & Responsibilities:  \n",
      " \n",
      "• Analysis, design, develops and Duty watch support SQL Production Database to meet client requirements.  \n",
      "• Understanding requirements and develop/changes standard productized SSIS packages, OLAP Cubes and reports.  \n",
      "• Understanding the behavior  of the data from various applications and insert/manage them accordingly in the data warehouse \n",
      "for further analysis and reporting purpose.  \n",
      "• Understanding the data warehouse and extract data for the reports to meet the requirement.  \n",
      "• Design the Cubes according to the requirement and fetch data using MDX queries for different reports.  \n",
      "• Developing reports or optimizes existing reports according to the requirement.  \n",
      "• Creating SSIS Packages to load the data from Excel sources to our data war ehouse  Using Transformations and Mappings in \n",
      "Data Flow Tasks  \n",
      "• Scheduled the ETL Package (Monthly) Using SQL Server 2012 Management Studio.  \n",
      "• Creating Dash Boards using SSAS Cubes and Power BI standalone models for Business users to access the model and analyz e \n",
      "the data.  \n",
      "• Created Pivot using Power Pivot, Geographical models using Power Maps and visualizations using Power View and Power \n",
      "Chart.  \n",
      "• Created so many dashboard reports, user parameters, slicers and other visualizations.  \n",
      "• Scheduling the Dashboards to refres h in Power Bi Desktop.  \n",
      "• Scheduling E -mail Driven SSRS reports on Weekly, Monthly basis  \n",
      " \n",
      " \n",
      " \n",
      "Project Title    :  Elite Warehouse  \n",
      "Tool/Language  :  Power BI, Object Query Language, SSIS,  SSAS,  MS-Access , Data Mapping Tool . \n",
      "Role                        :   MSBI Developer  \n",
      "Duration                :   Dec 2019 to June  2021 \n",
      " \n",
      "Project Description:  \n",
      " \n",
      "• The Company automates business processes and workflows within complex supply chain relationships. Our solutions include \n",
      "compliance, predictive analytics; Thomson deli vers Comprehensive Enterprise Applications powered by Business Intelligence \n",
      "solutions.  \n",
      "• Thomson Reuters and other claimants are now able to pursue the recovery of over a billion pounds in UK tax payments dating \n",
      "in most cases back to 1973 after a favorable ruling by the Supreme Court of England and Wales.  \n",
      " \n",
      "Roles & Responsibilities:  \n",
      "• Created and developed numerous simple to complex SQL procedures, function, joins Etc., handled complex business \n",
      "requirement  \n",
      "• Responsible for developing ETL data mapping documents  as well as data dictionaries as per Legacy system and 3E application.  \n",
      "• Maintained referential integrity, domain integrity and column integrity by using the available options such as check \n",
      "constraints, foreign key etc.  \n",
      "• Use Power Platform admin center and Power Apps  \n",
      "• Extensively worked as Data verse developer and Model -driven apps developer  \n",
      "• Development reports query like,  A/R, Claim, practice, Productivity & Analysis  \n",
      "• .Implementing Package Configuration to create setup and also Deployment at different environment like dev., stage, test and \n",
      "production  \n",
      "• Involved in Creating Visuals and Dashboards and Extensively worked on Power BI desktop and Power BI service  \n",
      "• Involved in interacting with business analyst for gathering  requirements and developed T -SQL code and SSIS Packages \n",
      "accordingly.  \n",
      "• Connect to your Git repos with SSH & Review and merge code with pull requests  \n",
      "• Involved in Azure Pipelines and YAML schema  \n",
      "• Involved in Azure Test Plans and  Run automated tests  \n",
      "• Use flows with Dataverse and Trigger flows with Dataverse events  \n",
      "• Custtomized using Power Automate Add a row,Delete a row and Upload or download files/images  \n",
      "• Create approval flows and Trigger approvals from SharePoint  \n",
      "• Customize responses and Use business process flow s \n",
      "• Create a business process flow and Add custom controls in business process flows  \n",
      "• Translated functional requirements into Technical specifications and Developed Stored Procedures and Complex SQL Queries.  • Created  ETL packages  using Heterogeneous data so urces like SQL Server, Flat Files, CSV Files, Excel source files, Access Files, \n",
      "XML files and then loaded the data into ODS/Staging areas by performing different kinds of transformations using SSIS Control  \n",
      "Flow and Data Flow transformations.  \n",
      "• Developed SSI S Packages using fuzzy lookup transformation , file system task, and for each  loop container.  \n",
      "• Applied various data transformations like Aggregate, Sort, Multicasting, Conditional Split, and Derived column, etc.  \n",
      "• Used Fuzzy Lookup and Fuzzy Grouping transformations to achieve higher degree of control over the data cleansing process \n",
      "such as standardizing data, correcting data, and providing missing values.  \n",
      "• Working knowledge on relational databases like SQL Server 2012/2008 R2/2005, T -SQL.  \n",
      "• Stabilization  – diagnostics and troubleshooting, bug fixing for the developed code.  \n",
      " \n",
      " \n",
      "Project Title            :  Health & Benefit Center standalone database  \n",
      "Tool/Language  :  Power BI, Object Query Language, SSIS, SSAS, MS -Access, Data Mapping Tool.  \n",
      "Role                        :  MSBI Developer  \n",
      "Duration                :   Dec 2018 to Nov 2019  \n",
      " \n",
      "Project Description:  \n",
      "• Care First , is a federation of 51 independently operated motor clubs throughout North America. Care first is a not-for-profit  \n",
      "member service organization  with more than 51 million members. It provides services to its members such as Medical, \n",
      "Electronics data.  \n",
      " \n",
      "Roles & Responsibilities:  \n",
      "• Analysis, design, develops and support application solutions to meet client requirements.  \n",
      "• Created databases and schema objects including tables, indexes and applied constraints, connected various applications to \n",
      "the database and written functions, stored procedures and triggers.  \n",
      "• Successfully migrated data between different heterogeneous sources such as flat file, Excel and SQL Server 2008 using SSIS, \n",
      "BCP and Bulk Insert.  \n",
      "• Created Power BI Datamodels, DAX and Visual s and Dashboards  \n",
      "• Used DAX and M to Create Variables and Transforms in Power BI Desktop and Power Query Editor  \n",
      "• Used Query Analyzer to optimize SQL Queries.  \n",
      "• Generated reports from the cubes by connecting to Analysis server from SSRS.  \n",
      "• Created Dimensions with the cube wizard and also individually.  \n",
      "• Designed, Developed and Deployed reports in MS SQL Server environment using SSRS -2008.  \n",
      "• Designed and created Report templates, bar graphs and pie charts based on the financial data.  \n",
      "• Created reports with  Analysis Services Cube as the data source using SQL Server 2005/2008 Reporting Services.  \n",
      "• Created the automated processes for the activities such as database backup processes and SSIS Packages run sequentially \n",
      "using SQL Server Agent job.  \n",
      "• Created Error and Performance reports on SSIS Packages, Jobs, Stored procedures and Triggers.  \n",
      "• Assisted in production of OLAP cubes, wrote queries to produce reports using SQL Server 2005/2008 Analysis Services (SSAS) \n",
      "and Reporting service (SSRS).  \n",
      "• Developed the SQL Serve r Integration Services (SSIS) packages to transform data from SQL 2005 to MS SQL 2008 as well as \n",
      "Created interface stored procedures used in SSIS to load/transform data to the database.  \n",
      "• Created Reports from Web Services and used MDX on Cubes in SSAS to cre ate reports in SSRS.  \n",
      "• Designed high level ETL architecture for overall data transfer from the OLTP to OLAP with the help of SSIS.  \n",
      "• Experience in using tools like index Tuning Wizard, SQL Profiler, and Windows Performance Monitor for Monitoring and \n",
      "Tuning MS SQL Server Performance.  \n",
      " \n",
      "Trainings Attended:  \n",
      "• Azure Data Engineering  \n",
      "• Python & R  \n",
      "• Spark & Scala  \n",
      " \n",
      "Certifications:  \n",
      "• Microsoft Power BI Data Analyst  \n",
      "• Microsoft Azure Fundamentals  \n",
      " \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    extractor = PdfParser(r\"C:\\Abhiraj\\NLPProject\\Resume\\Naveen Challa.pdf\")\n",
    "    extracted_text = extractor.extract_text()\n",
    "    print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Abhiraj Chaudhuri\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abhiraj', 'Chaudhuri']\n"
     ]
    }
   ],
   "source": [
    "word_tokenized_text = text.split(' ')\n",
    "print(word_tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 11113, 11961, 13006, 15775, 6784, 24572, 2072, 102]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Abhiraj\\NLPProject\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2004: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(r'C:\\Abhiraj\\NLPProject\\vocab.txt')\n",
    "encoding = tokenizer.encode(text)\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{101: '[CLS]', 11113: 'ab', 11961: '##hir', 13006: '##aj', 15775: 'cha', 6784: '##ud', 24572: '##hur', 2072: '##i', 102: '[SEP]'}\n"
     ]
    }
   ],
   "source": [
    "my_dict = {\n",
    "    101: \"[CLS]\",\n",
    "    11113: \"ab\",\n",
    "    11961: \"##hir\",\n",
    "    13006: \"##aj\",\n",
    "    15775: \"cha\",\n",
    "    6784: \"##ud\",\n",
    "    24572: \"##hur\",\n",
    "    2072: \"##i\",\n",
    "    102: \"[SEP]\",\n",
    "}\n",
    "print(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
